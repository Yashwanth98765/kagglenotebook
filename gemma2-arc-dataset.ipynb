{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"last_runtime":{"build_target":"//third_party/py/tunix/google/examples:colab_kernel","kind":"private"},"provenance":[]},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":914373,"sourceType":"datasetVersion","datasetId":491458},{"sourceId":7045423,"sourceType":"datasetVersion","datasetId":4054119,"isSourceIdPinned":false},{"sourceId":7086344,"sourceType":"datasetVersion","datasetId":4082890},{"sourceId":85992,"sourceType":"modelInstanceVersion","modelInstanceId":72251,"modelId":76277}],"dockerImageVersionId":31155,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GRPO Training on ARC Science Questions\n\nThis tutorial demonstrates training the [Gemma 2 2B-IT](https://deepmind.google/models/gemma/) model on the [ARC (AI2 Reasoning Challenge)](https://allenai.org/data/arc) science multiple-choice questions using [Group Relative Policy Optimization (GRPO)](https://arxiv.org/pdf/2402.03300).\n\nGRPO is an RL algorithm designed to enhance the reasoning abilities of LLMs. It is a variant of Proximal Policy Optimization (PPO) that uses group-based comparisons rather than a critic model.\n\n**Key Features:**\n- **Dataset**: ARC-Challenge (1119 training, 1172 test samples)\n- **Model**: Gemma-2 2B IT with LoRA (rank=64)\n- **Task**: Multiple choice science questions (A, B, C, D)\n- **Output Format**: `<reasoning>...</reasoning><answer>X</answer>`\n\n**Results achieved:**\n- Format Accuracy: **80.0%**\n- Answer Accuracy: **55.0%** (vs 25% random baseline)","metadata":{}},{"cell_type":"code","source":"!pip install -q wandb\n!pip install -q kagglehub\n\n!pip install -q ipywidgets\n\n!pip install -q tensorflow\n!pip install -q tensorflow_datasets\n!pip install -q tensorboardX\n!pip install -q transformers\n!pip install -q grain\n!pip install \"google-tunix[prod]==0.1.3\"\n\n!pip uninstall -q -y flax\n!pip install flax==0.12.0\n\n!pip install -q datasets wandb==0.22.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:52:57.46851Z","iopub.execute_input":"2026-01-10T17:52:57.468753Z","iopub.status.idle":"2026-01-10T17:53:46.851177Z","shell.execute_reply.started":"2026-01-10T17:52:57.468735Z","shell.execute_reply":"2026-01-10T17:53:46.850018Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Setup W&B for Experiment Tracking","metadata":{}},{"cell_type":"code","source":"import wandb, os\nfrom kaggle_secrets import UserSecretsClient\nos.environ['WANDB_API_KEY'] = UserSecretsClient().get_secret(\"WANDB_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:53:46.851727Z","iopub.execute_input":"2026-01-10T17:53:46.851894Z","iopub.status.idle":"2026-01-10T17:53:48.524886Z","shell.execute_reply.started":"2026-01-10T17:53:46.851877Z","shell.execute_reply":"2026-01-10T17:53:48.52388Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Imports\n\nImport all necessary libraries including JAX, Flax NNX, Tunix, and other dependencies.","metadata":{}},{"cell_type":"code","source":"import functools\nimport gc\nimport os\nfrom pprint import pprint\nimport re\nimport csv\nimport shutil\nimport json\n\nfrom flax import nnx\nimport grain\nimport humanize\nimport jax\nimport jax.numpy as jnp\nimport kagglehub\nimport optax\nfrom orbax import checkpoint as ocp\nfrom pathlib import Path\nimport qwix\nimport tensorflow_datasets as tfds\nfrom tqdm import tqdm\nimport numpy as np\n\n# Tunix imports\nfrom tunix.generate import sampler as sampler_lib\nfrom tunix.generate import tokenizer_adapter as tokenizer_lib\nfrom tunix.rl import rl_cluster as rl_cluster_lib\nfrom tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\nfrom tunix.rl.rollout import base_rollout\nfrom tunix.models.gemma import model as gemma_lib  # ‚Üê CHANGED!\nfrom tunix.models.gemma import params as params_lib\n\n# Metrics logger with fallback\ntry:\n    from tunix.training import metrics_logger\nexcept ModuleNotFoundError:\n    class metrics_logger:\n        class MetricsLoggerOptions:\n            def __init__(self, log_dir=None, flush_every_n_steps=20):\n                self.log_dir = log_dir\n                self.flush_every_n_steps = flush_every_n_steps\n\nprint(\"‚úÖ All imports successful!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:53:48.525263Z","iopub.execute_input":"2026-01-10T17:53:48.525437Z","iopub.status.idle":"2026-01-10T17:54:12.02853Z","shell.execute_reply.started":"2026-01-10T17:53:48.525422Z","shell.execute_reply":"2026-01-10T17:54:12.027342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check what's in gemma_lib\nimport tunix.models.gemma as gemma_module\nprint(\"Available in tunix.models.gemma:\")\nprint(dir(gemma_module))\n\n# Check submodules\nimport pkgutil\nprint(\"\\nSubmodules:\")\nfor importer, modname, ispkg in pkgutil.iter_modules(gemma_module.__path__):\n    print(f\"  {modname}\")\n    \n# Try to find Transformer\ntry:\n    from tunix.models.gemma.model import Transformer\n    print(\"\\n‚úÖ Found Transformer in tunix.models.gemma.model\")\nexcept:\n    print(\"\\n‚ùå Not in model\")\n    \ntry:\n    from tunix.models.gemma import gemma\n    print(\"In gemma submodule:\", dir(gemma))\nexcept:\n    print(\"No gemma submodule\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:54:12.029376Z","iopub.execute_input":"2026-01-10T17:54:12.02979Z","iopub.status.idle":"2026-01-10T17:54:12.03472Z","shell.execute_reply.started":"2026-01-10T17:54:12.029773Z","shell.execute_reply":"2026-01-10T17:54:12.033801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check what's in tunix.models.gemma\nimport tunix.models.gemma as gemma_pkg\nprint(\"Available in tunix.models.gemma:\")\nprint(dir(gemma_pkg))\n\n# Check submodules\nimport pkgutil\nprint(\"\\nSubmodules:\")\nfor importer, modname, ispkg in pkgutil.iter_modules(gemma_pkg.__path__):\n    print(f\"  {modname}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:54:12.035305Z","iopub.execute_input":"2026-01-10T17:54:12.035466Z","iopub.status.idle":"2026-01-10T17:54:12.047542Z","shell.execute_reply.started":"2026-01-10T17:54:12.035447Z","shell.execute_reply":"2026-01-10T17:54:12.046604Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Hyperparameters\n\nDefine the configuration for training:\n- **LoRA settings**: Rank=64, Alpha=64 for efficient fine-tuning\n- **GRPO settings**: Number of generations, KL penalty (beta=0.08), clipping (epsilon=0.2)\n- **Training settings**: Learning rate=3e-6, 3 epochs, 3357 total steps\n- **Cache settings**: Memory allocation for generation (768 tokens)","metadata":{}},{"cell_type":"code","source":"# ====== Data ======\nTRAIN_DATA_DIR = \"./data/train\"\nTEST_DATA_DIR = \"./data/test\"\nTRAIN_FRACTION = 1.0\n\n# ====== LoRA ======\nRANK = 64\nALPHA = 64.0\n\n# ====== Sharding ======\nMESH = [(1, 4), (\"fsdp\", \"tp\")]\n\n# ====== GRPO ======\nMAX_PROMPT_LENGTH = 256\nTOTAL_GENERATION_STEPS = 256\nTEMPERATURE = 0.9\nTOP_P = 1.0\nTOP_K = 50\nNUM_GENERATIONS = 2\nNUM_ITERATIONS = 1\nBETA = 0.08\nEPSILON = 0.2\n\n# ====== Training ======\nTRAIN_MICRO_BATCH_SIZE = 1\nNUM_BATCHES = 1119  # Number of ARC training samples\nNUM_TEST_BATCHES = 100\nEVAL_EVERY_N_STEPS = 50\nNUM_EPOCHS = 3\n\nMAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)\nprint(f\"MAX_STEPS = {MAX_STEPS}\")\n\n# === AdamW, warmup, cosine scheduler ===\nLEARNING_RATE = 3e-6\nB1 = 0.9\nB2 = 0.99\nWEIGHT_DECAY = 0.1\nWARMUP_STEPS = int(0.1 * MAX_STEPS)\nMAX_GRAD_NORM = 0.1\n\n# Checkpoint saving\nINTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\nCKPT_DIR = \"/tmp/content/ckpts/\"\nSAVE_INTERVAL_STEPS = 500\nMAX_TO_KEEP = 4\n\n# ====== Inference ======\nGENERATION_CONFIGS = {\n    \"greedy\": {\"temperature\": 0.0, \"top_k\": 1, \"top_p\": 1.0},\n    \"sampling\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n}\n\nprint(\"Hyperparameters configured!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:54:12.04803Z","iopub.execute_input":"2026-01-10T17:54:12.048189Z","iopub.status.idle":"2026-01-10T17:54:12.054952Z","shell.execute_reply.started":"2026-01-10T17:54:12.048176Z","shell.execute_reply":"2026-01-10T17:54:12.054103Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Utility Functions\n\nHelper function to monitor TPU memory usage.","metadata":{}},{"cell_type":"code","source":"def show_hbm_usage():\n    \"\"\"Displays memory usage per device.\"\"\"\n    fmt_size = functools.partial(humanize.naturalsize, binary=True)\n    for d in jax.local_devices():\n        stats = d.memory_stats()\n        used = stats[\"bytes_in_use\"]\n        limit = stats[\"bytes_limit\"]\n        print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({100*used/limit:.1f}%) on {d}\")\n\nshow_hbm_usage()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:54:12.055442Z","iopub.execute_input":"2026-01-10T17:54:12.055607Z","iopub.status.idle":"2026-01-10T17:54:21.248134Z","shell.execute_reply.started":"2026-01-10T17:54:12.055594Z","shell.execute_reply":"2026-01-10T17:54:21.246876Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Preprocessing - Output Format\n\nWe define special tokens for structured output. The model is instructed to:\n1. **Reason** between `<reasoning>` and `</reasoning>` tags\n2. **Answer** with a single letter (A, B, C, or D) between `<answer>` and `</answer>` tags\n\nThis format encourages step-by-step reasoning before selecting an answer.","metadata":{}},{"cell_type":"code","source":"reasoning_start = \"<reasoning>\"\nreasoning_end = \"</reasoning>\"\nsolution_start = \"<answer>\"\nsolution_end = \"</answer>\"\n\n# SYSTEM PROMPT for ARC Science Questions\nSYSTEM_PROMPT = f\"\"\"You are a helpful assistant that solves multiple choice science questions.\n\nFor each question:\n1. Read the question and all answer choices carefully\n2. Think through the problem step by step inside {reasoning_start} and {reasoning_end} tags\n3. Provide your final answer (A, B, C, or D) inside {solution_start} and {solution_end} tags\n\nExample:\n{reasoning_start}\nLet me analyze each option:\n- Option A: [analysis]\n- Option B: [analysis]\n- Option C: [analysis]\n- Option D: [analysis]\nBased on my analysis, the correct answer is...\n{reasoning_end}\n{solution_start}B{solution_end}\n\"\"\"\n\nTEMPLATE = \"\"\"<start_of_turn>user\n{system_prompt}\n\nQuestion: {question}<end_of_turn>\n<start_of_turn>model\n\"\"\"\n\nprint(\"System prompt configured for ARC dataset!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:54:21.248912Z","iopub.execute_input":"2026-01-10T17:54:21.249101Z","iopub.status.idle":"2026-01-10T17:54:21.253679Z","shell.execute_reply.started":"2026-01-10T17:54:21.249084Z","shell.execute_reply":"2026-01-10T17:54:21.252855Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load ARC Dataset\n\nWe use the [ARC (AI2 Reasoning Challenge)](https://allenai.org/data/arc) dataset containing grade-school level science questions. Each question has 4 multiple choice options.\n\nThe dataset tests scientific reasoning across topics like:\n- Physics (forces, energy, motion)\n- Biology (cells, organisms, ecosystems)\n- Chemistry (matter, reactions)\n- Earth Science (weather, geology)","metadata":{}},{"cell_type":"code","source":"# === Load ARC Dataset ===\ndef load_arc_jsonl(filepath):\n    \"\"\"Load ARC JSONL file\"\"\"\n    data = []\n    with open(filepath, 'r') as f:\n        for line in f:\n            data.append(json.loads(line))\n    return data\n\ndef format_arc_question(item):\n    \"\"\"Format ARC item into question string\"\"\"\n    stem = item['question']['stem']\n    choices = item['question']['choices']\n    \n    choice_str = \"\"\n    for choice in choices:\n        choice_str += f\"{choice['label']}: {choice['text']}\\n\"\n    \n    return f\"{stem}\\n\\n{choice_str}\".strip()\n\ndef get_arc_answer(item):\n    \"\"\"Get the answer key\"\"\"\n    return item['answerKey']\n\n# Load training and test data\ntrain_data_raw = load_arc_jsonl('/kaggle/input/arc-ai2-reasoning-challenge/ARC-Challenge-Train.jsonl')\ntest_data_raw = load_arc_jsonl('/kaggle/input/arc-ai2-reasoning-challenge/ARC-Challenge-Test.jsonl')\n\nprint(f\"Loaded {len(train_data_raw)} training samples\")\nprint(f\"Loaded {len(test_data_raw)} test samples\")\n\n# Format all data into simple list\ntrain_dataset = []\nfor item in train_data_raw:\n    train_dataset.append({\n        'prompt': format_arc_question(item),\n        'answer': get_arc_answer(item)\n    })\n\ntest_dataset = []\nfor item in test_data_raw:\n    test_dataset.append({\n        'prompt': format_arc_question(item),\n        'answer': get_arc_answer(item)\n    })\n\nprint(f\"\\nFormatted {len(train_dataset)} training samples\")\nprint(f\"Formatted {len(test_dataset)} test samples\")\n\n# Show example\nprint(\"\\n\" + \"=\"*50)\nprint(\"EXAMPLE QUESTION:\")\nprint(train_dataset[0]['prompt'])\nprint(\"\\nANSWER:\", train_dataset[0]['answer'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:54:21.254376Z","iopub.execute_input":"2026-01-10T17:54:21.25456Z","iopub.status.idle":"2026-01-10T17:54:21.532539Z","shell.execute_reply.started":"2026-01-10T17:54:21.254546Z","shell.execute_reply":"2026-01-10T17:54:21.531387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"source = 'arc'\nprint(f\"Using data source: {source}\")\n\n# Prepare raw data list\narc_raw_data = []\nfor item in train_dataset:\n    arc_raw_data.append({\n        \"question\": item['prompt'],\n        \"answer\": item['answer'],\n    })\n\nprint(f\"Prepared {len(arc_raw_data)} raw training samples\")\n\n# Create the grain.MapDataset EXACTLY like original GSM8K notebook\ndataset = (\n    grain.MapDataset.source(arc_raw_data)\n    .shuffle(seed=42)\n    .map(\n        lambda x: {\n            \"prompts\": TEMPLATE.format(\n                system_prompt=SYSTEM_PROMPT,\n                question=x[\"question\"],\n            ),\n            \"question\": x[\"question\"],\n            \"answer\": x[\"answer\"],\n        }\n    )\n)\n\n# Batch it\ndataset = dataset.batch(TRAIN_MICRO_BATCH_SIZE)[:NUM_BATCHES]\n\n# Repeat for epochs\ntrain_dataset_final = dataset.repeat(NUM_EPOCHS)\n\nprint(f\"‚úÖ Created train_dataset_final with {len(train_dataset_final)} batches\")\n\n# Verify first batch\nfor batch in train_dataset_final[:1]:\n    print(f\"‚úÖ First batch keys: {batch.keys()}\")\n    print(f\"‚úÖ Prompts type: {type(batch['prompts'])}\")\n    if len(batch['prompts']) > 0:\n        print(f\"‚úÖ First prompt type: {type(batch['prompts'][0])}\")\n        first_prompt = batch['prompts'][0]\n        if hasattr(first_prompt, 'decode'):\n            first_prompt = first_prompt.decode('utf-8')\n        print(f\"‚úÖ First prompt (100 chars): {first_prompt[:100]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:54:21.533165Z","iopub.execute_input":"2026-01-10T17:54:21.53334Z","iopub.status.idle":"2026-01-10T17:54:21.545571Z","shell.execute_reply.started":"2026-01-10T17:54:21.533324Z","shell.execute_reply":"2026-01-10T17:54:21.544777Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preview Training Data\n\nLet's see how one batch of the training dataset looks like!","metadata":{}},{"cell_type":"code","source":"# Show example batch\nprint(\"Example training batch:\")\nif len(dataset) > 0:\n    print(f\"Prompts: {dataset[0]['prompts'][0][:200]}...\")\n    print(f\"Answer: {dataset[0]['answer'][0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:54:21.546043Z","iopub.execute_input":"2026-01-10T17:54:21.546203Z","iopub.status.idle":"2026-01-10T17:54:21.552339Z","shell.execute_reply.started":"2026-01-10T17:54:21.546189Z","shell.execute_reply":"2026-01-10T17:54:21.551486Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Kaggle Authentication\n\nLog in to Kaggle to download the Gemma model weights.","metadata":{}},{"cell_type":"code","source":"# Log in to Kaggle\nif \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n    kagglehub.login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:54:21.552924Z","iopub.execute_input":"2026-01-10T17:54:21.553082Z","iopub.status.idle":"2026-01-10T17:54:21.566662Z","shell.execute_reply.started":"2026-01-10T17:54:21.553068Z","shell.execute_reply":"2026-01-10T17:54:21.565875Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Download Gemma Model\n\nDownload the Gemma 2 2B-IT model from Kaggle.","metadata":{}},{"cell_type":"code","source":"model_path = {\n    \"gemma2\": \"google/gemma-2/flax/\",\n}\nmodel_family = \"gemma2\"\nmodel_version = \"gemma2-2b-it\"\nprint(f\"{model_path[model_family]}{model_version}\")\n\nkaggle_ckpt_path = kagglehub.model_download(\n    f\"{model_path[model_family]}{model_version}\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:54:21.567149Z","iopub.execute_input":"2026-01-10T17:54:21.56731Z","iopub.status.idle":"2026-01-10T17:54:22.205836Z","shell.execute_reply.started":"2026-01-10T17:54:21.567297Z","shell.execute_reply":"2026-01-10T17:54:22.204573Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Checkpoint Conversion\n\nRe-save the pre-trained Gemma checkpoint into a format compatible with Flax NNX. The original Kaggle checkpoint has parameter names that need to be reformatted.","metadata":{}},{"cell_type":"code","source":"# Re-save checkpoint for NNX compatibility\n!rm /tmp/content/intermediate_ckpt/* -rf\n!rm /tmp/content/ckpts/* -rf\n\nif model_family == \"gemma2\":\n    params = params_lib.load_and_format_params(\n        os.path.join(kaggle_ckpt_path, \"gemma2-2b-it\")\n    )\n    gemma = gemma_lib.Transformer.from_params(params, version=\"2-2b-it\")\n    checkpointer = ocp.StandardCheckpointer()\n    _, state = nnx.split(gemma)\n    checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)\n    checkpointer.wait_until_finished()\n    del params\n    del gemma\n    del state\n    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:54:22.20682Z","iopub.execute_input":"2026-01-10T17:54:22.207008Z","iopub.status.idle":"2026-01-10T17:55:14.23158Z","shell.execute_reply.started":"2026-01-10T17:54:22.20699Z","shell.execute_reply":"2026-01-10T17:55:14.230099Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Loading Functions\n\nTwo key functions:\n- **`get_gemma_ref_model`**: Loads the Gemma model with JAX sharding for multi-device distribution\n- **`get_lora_model`**: Applies LoRA layers to attention and MLP modules for efficient training\n\nThe **reference model** stays frozen and is used to compute KL divergence, ensuring the policy doesn't deviate too far from the original behavior.","metadata":{}},{"cell_type":"code","source":"def get_gemma_ref_model(ckpt_path):\n    mesh = jax.make_mesh(*MESH)\n    model_config = gemma_lib.ModelConfig.gemma2_2b()\n    abs_gemma: nnx.Module = nnx.eval_shape(\n        lambda: gemma_lib.Transformer(model_config, rngs=nnx.Rngs(params=0))\n    )\n    abs_state = nnx.state(abs_gemma)\n    abs_state = jax.tree.map(\n        lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n        abs_state,\n        nnx.get_named_sharding(abs_state, mesh),\n    )\n    checkpointer = ocp.StandardCheckpointer()\n    restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n    graph_def, _ = nnx.split(abs_gemma)\n    gemma = nnx.merge(graph_def, restored_params)\n    return gemma, mesh, model_config\n\n\ndef get_lora_model(base_model, mesh):\n    lora_provider = qwix.LoraProvider(\n        module_path=(\n            \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n            \".*attn_vec_einsum\"\n        ),\n        rank=RANK,\n        alpha=ALPHA,\n    )\n    model_input = base_model.get_model_input()\n    lora_model = qwix.apply_lora_to_model(\n        base_model, lora_provider, **model_input\n    )\n    with mesh:\n        state = nnx.state(lora_model)\n        pspecs = nnx.get_partition_spec(state)\n        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n        nnx.update(lora_model, sharded_state)\n    return lora_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:55:14.23204Z","iopub.execute_input":"2026-01-10T17:55:14.23227Z","iopub.status.idle":"2026-01-10T17:55:14.238095Z","shell.execute_reply.started":"2026-01-10T17:55:14.232251Z","shell.execute_reply":"2026-01-10T17:55:14.237204Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load Reference Model\n\nLoad the base Gemma model that will serve as our reference for KL divergence calculation.","metadata":{}},{"cell_type":"code","source":"# Reference model\nif model_family == \"gemma2\":\n    ref_model, mesh, model_config = get_gemma_ref_model(\n        ckpt_path=os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:55:14.238658Z","iopub.execute_input":"2026-01-10T17:55:14.238835Z","iopub.status.idle":"2026-01-10T17:55:17.405613Z","shell.execute_reply.started":"2026-01-10T17:55:14.238819Z","shell.execute_reply":"2026-01-10T17:55:17.404223Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load Policy Model (LoRA)\n\nApply LoRA adapters to create the trainable policy model. Only LoRA parameters will be updated during training.","metadata":{}},{"cell_type":"code","source":"# Policy model\nlora_policy = get_lora_model(ref_model, mesh=mesh)\nnnx.display(lora_policy)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:55:17.4061Z","iopub.execute_input":"2026-01-10T17:55:17.406293Z","iopub.status.idle":"2026-01-10T17:55:54.96261Z","shell.execute_reply.started":"2026-01-10T17:55:17.406277Z","shell.execute_reply":"2026-01-10T17:55:54.96134Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load Tokenizer\n\nLoad the SentencePiece tokenizer and wrap it with TokenizerAdapter for Tunix compatibility.","metadata":{}},{"cell_type":"code","source":"# Load tokenizer using sentencepiece directly, then wrap it\nimport sentencepiece as spm\n\n# Load the sentencepiece model\nsp_model = spm.SentencePieceProcessor()\nsp_model.Load(os.path.join(kaggle_ckpt_path, \"tokenizer.model\"))\n\n# Wrap with TokenizerAdapter\ntokenizer = tokenizer_lib.TokenizerAdapter(sp_model)\n\nprint(\"‚úÖ Tokenizer loaded!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:55:54.963142Z","iopub.execute_input":"2026-01-10T17:55:54.963328Z","iopub.status.idle":"2026-01-10T17:55:55.079429Z","shell.execute_reply.started":"2026-01-10T17:55:54.963312Z","shell.execute_reply":"2026-01-10T17:55:55.078199Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Format Matching Regex\n\nDefine regex pattern to validate the model's output format.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Regex for format matching\nmatch_format = re.compile(\n    rf\"^[\\s]{{0,}}\"\n    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\n    rf\"{solution_start}(.+?){solution_end}\"\n    rf\"[\\s]{{0,}}$\",\n    flags=re.MULTILINE | re.DOTALL,\n)\n\n# Test the regex\nresult = match_format.search(\n    f\"{reasoning_start}Let me think through this step by step...{reasoning_end}{solution_start}B{solution_end}\",\n)\nprint(f\"Regex test: {result is not None}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:55:55.080002Z","iopub.execute_input":"2026-01-10T17:55:55.080187Z","iopub.status.idle":"2026-01-10T17:55:55.089709Z","shell.execute_reply.started":"2026-01-10T17:55:55.08017Z","shell.execute_reply":"2026-01-10T17:55:55.088815Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Define Reward Functions\n\nWe define four reward functions to guide GRPO training:\n\n1. **`match_format_exactly`** (3 points): Full reward if output has both `<reasoning>` and `<answer>` tags\n2. **`match_format_approximately`** (partial): Incremental rewards for partial format compliance  \n3. **`check_reasoning_quality`** (up to 3 points): Rewards for analyzing options and using reasoning words\n4. **`check_answer_correct`** (5 points): Big reward for selecting the correct answer letter (A/B/C/D)\n\nThese reward functions shape the model to produce well-structured, reasoned responses.","metadata":{}},{"cell_type":"code","source":"def match_format_exactly(prompts, completions, **kwargs):\n    \"\"\"Reward if the format matches exactly (3 points).\"\"\"\n    scores = []\n    for response in completions:\n        # Convert bytes to string if needed\n        if hasattr(response, 'decode'):\n            response = response.decode('utf-8')\n        has_reasoning = bool(re.search(r'<reasoning>.*?</reasoning>', str(response), re.DOTALL))\n        has_answer = bool(re.search(r'<answer>.*?</answer>', str(response), re.DOTALL))\n        if has_reasoning and has_answer:\n            scores.append(3.0)\n        else:\n            scores.append(0.0)\n    return scores\n\n\ndef match_format_approximately(prompts, completions, **kwargs):\n    \"\"\"Reward partial format matches.\"\"\"\n    scores = []\n    for response in completions:\n        if hasattr(response, 'decode'):\n            response = response.decode('utf-8')\n        response = str(response)\n        score = 0\n        if '<reasoning>' in response:\n            score += 1.0\n        if '</reasoning>' in response:\n            score += 0.5\n        if '<answer>' in response:\n            score += 1.0\n        if '</answer>' in response:\n            score += 0.5\n        scores.append(score - 1.5)\n    return scores\n\n\ndef check_reasoning_quality(prompts, completions, answer=None, **kwargs):\n    \"\"\"Reward for quality reasoning indicators.\"\"\"\n    scores = []\n    for response in completions:\n        if hasattr(response, 'decode'):\n            response = response.decode('utf-8')\n        response = str(response)\n        score = 0.0\n        \n        # Check for option analysis\n        for opt in ['A', 'B', 'C', 'D']:\n            if f\"Option {opt}\" in response or f\"{opt}:\" in response:\n                score += 0.5\n        \n        # Check for reasoning words\n        reasoning_words = ['because', 'therefore', 'since', 'means', 'correct', 'incorrect', 'wrong', 'right']\n        for word in reasoning_words:\n            if word.lower() in response.lower():\n                score += 0.25\n        \n        scores.append(min(score, 3.0))\n    return scores\n\n\ndef check_answer_correct(prompts, completions, answer=None, **kwargs):\n    \"\"\"Check if the answer letter is correct - BIG reward!\"\"\"\n    scores = []\n    \n    # Handle numpy array\n    if answer is not None and hasattr(answer, 'tolist'):\n        answer = answer.tolist()\n    \n    for i, response in enumerate(completions):\n        # Convert bytes to string if needed\n        if hasattr(response, 'decode'):\n            response = response.decode('utf-8')\n        response = str(response)\n        \n        # Get reference answer\n        ref_answer = None\n        if answer is not None:\n            if isinstance(answer, list) and i < len(answer):\n                ref_answer = answer[i]\n            elif isinstance(answer, str):\n                ref_answer = answer\n        \n        # Convert ref_answer to string\n        if ref_answer is not None:\n            if hasattr(ref_answer, 'decode'):\n                ref_answer = ref_answer.decode('utf-8')\n            ref_answer = str(ref_answer)\n        \n        # Extract answer from <answer> tags\n        match = re.search(r'<answer>\\s*([A-Da-d])\\s*</answer>', response)\n        \n        if match:\n            extracted = match.group(1).upper()\n            if ref_answer and extracted == ref_answer.upper():\n                scores.append(5.0)  # Correct answer!\n            else:\n                scores.append(1.0)  # Has answer format but wrong/can't verify\n        else:\n            scores.append(0.0)  # No answer found\n    return scores\n\n\nprint(\"Reward functions defined!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:55:55.090138Z","iopub.execute_input":"2026-01-10T17:55:55.090303Z","iopub.status.idle":"2026-01-10T17:55:55.099319Z","shell.execute_reply.started":"2026-01-10T17:55:55.090289Z","shell.execute_reply":"2026-01-10T17:55:55.098438Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generation Function\n\nHelper function to generate model responses given a question prompt.","metadata":{}},{"cell_type":"code","source":"def generate(question, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None):\n    \"\"\"Given prompt, generates text.\"\"\"\n    if isinstance(question, str):\n        input_batch = [\n            TEMPLATE.format(\n                system_prompt=SYSTEM_PROMPT,\n                question=question,\n            )\n        ]\n    else:\n        input_batch = [\n            TEMPLATE.format(\n                system_prompt=SYSTEM_PROMPT,\n                question=q,\n            )\n            for q in question\n        ]\n\n    out_data = sampler(\n        input_strings=input_batch,\n        max_generation_steps=TOTAL_GENERATION_STEPS,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        echo=False,\n        seed=seed if seed is not None else None,\n    )\n\n    output = out_data.text\n    if isinstance(question, str):\n        return output[0]\n    return output\n\nprint(\"Generate function defined!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:55:55.099716Z","iopub.execute_input":"2026-01-10T17:55:55.099872Z","iopub.status.idle":"2026-01-10T17:55:55.108805Z","shell.execute_reply.started":"2026-01-10T17:55:55.099859Z","shell.execute_reply":"2026-01-10T17:55:55.107939Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation Functions\n\nFunctions to extract answers and evaluate model accuracy on the test set.","metadata":{}},{"cell_type":"code","source":"def extract_answer(response):\n    \"\"\"Extract answer letter from response\"\"\"\n    match = re.search(r'<answer>\\s*([A-Da-d])\\s*</answer>', response)\n    if match:\n        return match.group(1).upper()\n    return None\n\ndef check_format(response):\n    \"\"\"Check if response has proper format\"\"\"\n    has_reasoning = '<reasoning>' in response and '</reasoning>' in response\n    has_answer = '<answer>' in response and '</answer>' in response\n    return has_reasoning and has_answer\n\ndef evaluate(dataset, sampler, temperature=0.7, top_k=50, top_p=0.95, num_samples=50):\n    \"\"\"Evaluates the model on ARC questions.\"\"\"\n    corr_format = 0\n    corr_answer = 0\n    total = 0\n    \n    eval_samples = dataset[:min(num_samples, len(dataset))]\n    \n    for i, item in enumerate(tqdm(eval_samples)):\n        question = item['prompt']\n        ref_answer = item['answer']\n        \n        response = generate(question, sampler, temperature, top_k, top_p)\n        \n        if check_format(response):\n            corr_format += 1\n        \n        extracted = extract_answer(response)\n        if extracted and extracted == ref_answer.upper():\n            corr_answer += 1\n        \n        total += 1\n        \n        if (i + 1) % 10 == 0:\n            print(f\"Progress: {total}/{len(eval_samples)}, Format: {100*corr_format/total:.1f}%, Correct: {100*corr_answer/total:.1f}%\")\n    \n    format_accuracy = 100 * corr_format / total if total > 0 else 0\n    answer_accuracy = 100 * corr_answer / total if total > 0 else 0\n    \n    return corr_format, total, format_accuracy, answer_accuracy\n\nprint(\"Evaluate function defined!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:55:55.109084Z","iopub.execute_input":"2026-01-10T17:55:55.109235Z","iopub.status.idle":"2026-01-10T17:55:55.119084Z","shell.execute_reply.started":"2026-01-10T17:55:55.109221Z","shell.execute_reply":"2026-01-10T17:55:55.118258Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create Sampler\n\nCreate the sampler for text generation with the appropriate cache configuration.","metadata":{}},{"cell_type":"code","source":"sampler = sampler_lib.Sampler(\n    transformer=lora_policy,\n    tokenizer=tokenizer,\n    cache_config=sampler_lib.CacheConfig(\n        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n        num_layers=model_config.num_layers,\n        num_kv_heads=model_config.num_kv_heads,\n        head_dim=model_config.head_dim,\n    ),\n)\nprint(\"Sampler created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:55:55.119497Z","iopub.execute_input":"2026-01-10T17:55:55.119674Z","iopub.status.idle":"2026-01-10T17:55:55.172982Z","shell.execute_reply.started":"2026-01-10T17:55:55.11966Z","shell.execute_reply":"2026-01-10T17:55:55.17205Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Pre-Training Evaluation (Baseline)\n\nEvaluate the base model **before training** to establish a baseline. This helps us measure improvement after GRPO training.\n\nExpected: ~0% format accuracy (model doesn't know the format yet)","metadata":{}},{"cell_type":"code","source":"# Evaluate before training\nprint(\"Pre-training evaluation...\")\n(corr_format, total, format_accuracy, answer_accuracy) = evaluate(\n    test_dataset,\n    sampler,\n    **GENERATION_CONFIGS[\"greedy\"],\n    num_samples=30\n)\nprint(f\"\\nPRE-TRAINING RESULTS:\")\nprint(f\"  Total samples: {total}\")\nprint(f\"  Format accuracy: {format_accuracy:.1f}%\")\nprint(f\"  Answer accuracy: {answer_accuracy:.1f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:55:55.173384Z","iopub.execute_input":"2026-01-10T17:55:55.173541Z","iopub.status.idle":"2026-01-10T17:57:10.013762Z","shell.execute_reply.started":"2026-01-10T17:55:55.173527Z","shell.execute_reply":"2026-01-10T17:57:10.012525Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Configuration\n\nSetting up:\n- **Checkpointing**: Save model every 500 steps, keep last 4 checkpoints\n- **Metrics Logging**: Log to TensorBoard\n- **Optimizer**: AdamW with warmup and cosine decay schedule","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Checkpoint saving options\ncheckpointing_options = ocp.CheckpointManagerOptions(\n    save_interval_steps=SAVE_INTERVAL_STEPS, max_to_keep=MAX_TO_KEEP\n)\n\n# Metrics logger\nmetrics_logging_options = metrics_logger.MetricsLoggerOptions(\n    log_dir=\"/tmp/content/tmp/tensorboard/grpo\", flush_every_n_steps=20\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:57:10.014325Z","iopub.execute_input":"2026-01-10T17:57:10.014509Z","iopub.status.idle":"2026-01-10T17:57:10.017924Z","shell.execute_reply.started":"2026-01-10T17:57:10.014494Z","shell.execute_reply":"2026-01-10T17:57:10.017131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Optimizer with warmup and cosine decay\noptimizer = optax.adamw(\n    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n        init_value=0.0,\n        peak_value=LEARNING_RATE,\n        warmup_steps=WARMUP_STEPS,\n        decay_steps=MAX_STEPS,\n        end_value=0.0,\n    ),\n    b1=B1,\n    b2=B2,\n    weight_decay=WEIGHT_DECAY,\n)\nif MAX_GRAD_NORM is not None:\n    optimizer = optax.chain(\n        optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n        optimizer,\n    )\nprint(\"Optimizer configured!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:57:10.01846Z","iopub.execute_input":"2026-01-10T17:57:10.018618Z","iopub.status.idle":"2026-01-10T17:57:10.029852Z","shell.execute_reply.started":"2026-01-10T17:57:10.018605Z","shell.execute_reply":"2026-01-10T17:57:10.028956Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cluster Configuration\n\nConfigure the distributed training setup:\n- **Mesh**: Device mesh for FSDP and tensor parallelism\n- **Rollout Config**: Generation parameters (temperature, top-k, top-p)\n- **Training Config**: Batch sizes, learning rate, max steps","metadata":{}},{"cell_type":"code","source":"# Training config - CONSISTENT cache sizes!\nKV_CACHE_SIZE = MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256\nprint(f\"Using KV_CACHE_SIZE = {KV_CACHE_SIZE}\")\n\ncluster_config = rl_cluster_lib.ClusterConfig(\n    role_to_mesh={\n        rl_cluster_lib.Role.ACTOR: mesh,\n        rl_cluster_lib.Role.REFERENCE: mesh,\n        rl_cluster_lib.Role.ROLLOUT: mesh,\n    },\n    rollout_engine='vanilla',\n    offload_to_cpu=False,\n    training_config=rl_cluster_lib.RLTrainingConfig(\n        actor_optimizer=optimizer,\n        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n        max_steps=MAX_STEPS,\n        mini_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        train_micro_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        metrics_logging_options=metrics_logging_options,\n        checkpoint_root_directory=CKPT_DIR,\n        checkpointing_options=checkpointing_options,\n    ),\n    rollout_config=base_rollout.RolloutConfig(\n        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n        max_prompt_length=MAX_PROMPT_LENGTH,\n        kv_cache_size=KV_CACHE_SIZE,\n        temperature=TEMPERATURE,\n        top_p=TOP_P,\n        top_k=TOP_K,\n    ),\n)\n\ngrpo_config = GRPOConfig(\n    num_generations=NUM_GENERATIONS,\n    num_iterations=NUM_ITERATIONS,\n    beta=BETA,\n    epsilon=EPSILON,\n)\nprint(\"Cluster config created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:57:10.030282Z","iopub.execute_input":"2026-01-10T17:57:10.030445Z","iopub.status.idle":"2026-01-10T17:57:10.037831Z","shell.execute_reply.started":"2026-01-10T17:57:10.030425Z","shell.execute_reply":"2026-01-10T17:57:10.036986Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Initialize GRPO Trainer\n\nCreate the training components:\n1. **RLCluster**: Combines the policy (LoRA model), reference model, and tokenizer\n2. **GRPOLearner**: The trainer that uses our reward functions to optimize the model\n\nThe trainer generates multiple responses per prompt and uses relative rewards to update the policy.","metadata":{}},{"cell_type":"code","source":"# RL cluster and GRPO Trainer\nrl_cluster = rl_cluster_lib.RLCluster(\n    actor=lora_policy,\n    reference=ref_model,\n    tokenizer=tokenizer,\n    cluster_config=cluster_config,\n)\n\n# GRPO Trainer with ARC reward functions\ngrpo_trainer = GRPOLearner(\n    rl_cluster=rl_cluster,\n    reward_fns=[\n        match_format_exactly,\n        match_format_approximately,\n        check_reasoning_quality,\n        check_answer_correct,\n    ],\n    grpo_config=grpo_config,\n)\nprint(\"GRPO Trainer created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:57:10.038159Z","iopub.execute_input":"2026-01-10T17:57:10.038313Z","iopub.status.idle":"2026-01-10T17:57:18.210837Z","shell.execute_reply.started":"2026-01-10T17:57:10.0383Z","shell.execute_reply":"2026-01-10T17:57:18.209673Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Start Training! üöÄ\n\nTrain the model using GRPO. The first few steps may take longer due to JIT compilation.\n\nTraining for **3357 steps** (~53 minutes on TPU v5e)\n\nMetrics logged to W&B:\n- Loss, KL divergence, Perplexity\n- Steps per second, TFLOPs","metadata":{}},{"cell_type":"code","source":"# Start training!\nprint(f\"Starting training for {MAX_STEPS} steps...\")\nwith mesh:\n    grpo_trainer.train(train_dataset_final)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:57:18.211421Z","iopub.execute_input":"2026-01-10T17:57:18.211656Z","iopub.status.idle":"2026-01-10T18:52:24.463284Z","shell.execute_reply.started":"2026-01-10T17:57:18.211627Z","shell.execute_reply":"2026-01-10T18:52:24.462074Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test Trained Model\n\nLet's test the trained model with a simple question to verify it generates the expected format with proper reasoning and answer tags.","metadata":{}},{"cell_type":"code","source":"# # SIMPLE TEST - See what the model actually generates\n# import wandb\n# import os\n# os.environ['WANDB_MODE'] = 'disabled'  # Disable wandb completely\n\n# # Create a simple sampler\n# test_sampler = sampler_lib.Sampler(\n#     transformer=lora_policy,\n#     tokenizer=tokenizer,\n#     cache_config=sampler_lib.CacheConfig(\n#         cache_size=512,\n#         num_layers=model_config.num_layers,\n#         num_kv_heads=model_config.num_kv_heads,\n#         head_dim=model_config.head_dim,\n#     ),\n# )\n\n# # Test with ONE simple question\n# test_question = \"Which is a renewable resource?\\n\\nA: coal\\nB: oil\\nC: sunlight\\nD: natural gas\"\n\n# prompt = TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=test_question)\n\n# print(\"INPUT PROMPT:\")\n# print(\"=\"*60)\n# print(prompt)\n# print(\"=\"*60)\n\n# # Generate\n# out = test_sampler(\n#     input_strings=[prompt],\n#     max_generation_steps=200,\n#     temperature=0.0,\n# )\n\n# print(\"\\nRAW MODEL OUTPUT:\")\n# print(\"=\"*60)\n# print(out.text[0])\n# print(\"=\"*60)\n\n# SIMPLE TEST - See what the model actually generates\nimport os\n# MUST set this BEFORE importing wandb\nos.environ['WANDB_MODE'] = 'disabled'\nos.environ['WANDB_DISABLED'] = 'true'\n\n# Re-initialize wandb in disabled mode (or skip entirely)\ntry:\n    import wandb\n    wandb.init(mode=\"disabled\")\nexcept:\n    pass\n\n# Create a simple sampler\ntest_sampler = sampler_lib.Sampler(\n    transformer=lora_policy,\n    tokenizer=tokenizer,\n    cache_config=sampler_lib.CacheConfig(\n        cache_size=768,  # Increased cache size\n        num_layers=model_config.num_layers,\n        num_kv_heads=model_config.num_kv_heads,\n        head_dim=model_config.head_dim,\n    ),\n)\n\n# Test with ONE simple question\ntest_question = \"Which is a renewable resource?\\n\\nA: coal\\nB: oil\\nC: sunlight\\nD: natural gas\"\nprompt = TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=test_question)\n\nprint(\"INPUT PROMPT:\")\nprint(\"=\"*60)\nprint(prompt)\nprint(\"=\"*60)\n\n# Generate\nout = test_sampler(\n    input_strings=[prompt],\n    max_generation_steps=200,\n    temperature=0.0,\n)\n\nprint(\"\\nRAW MODEL OUTPUT:\")\nprint(\"=\"*60)\nprint(out.text[0])\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T18:52:24.463839Z","iopub.execute_input":"2026-01-10T18:52:24.46404Z","iopub.status.idle":"2026-01-10T18:52:48.909331Z","shell.execute_reply.started":"2026-01-10T18:52:24.464024Z","shell.execute_reply":"2026-01-10T18:52:48.908183Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save Trained Model\n\nSave the LoRA parameters so we can reload the trained model later without retraining.\n\nThe model is saved to `/kaggle/working/trained_arc_model/lora_params`","metadata":{}},{"cell_type":"code","source":"# SAVE TRAINED MODEL\nimport os\n\n# Save the LoRA weights\nsave_path = \"/kaggle/working/trained_arc_model\"\nos.makedirs(save_path, exist_ok=True)\n\n# Save LoRA parameters\nlora_state = nnx.state(lora_policy, nnx.LoRAParam)\ncheckpointer = ocp.StandardCheckpointer()\ncheckpointer.save(os.path.join(save_path, \"lora_params\"), lora_state)\ncheckpointer.wait_until_finished()\n\nprint(f\"‚úÖ Model saved to {save_path}\")\nprint(\"You can download this from Kaggle's Output tab!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T18:52:48.909846Z","iopub.execute_input":"2026-01-10T18:52:48.910014Z","iopub.status.idle":"2026-01-10T18:52:50.615848Z","shell.execute_reply.started":"2026-01-10T18:52:48.909998Z","shell.execute_reply":"2026-01-10T18:52:50.614681Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load Trained Model\n\nDemonstrate how to reload the saved LoRA parameters into the model.","metadata":{}},{"cell_type":"code","source":"# LOAD TRAINED MODEL (after setting up base model)\nload_path = \"/kaggle/working/trained_arc_model\"\n\n# Load saved LoRA params\nabs_params = jax.tree.map(\n    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n    nnx.state(lora_policy, nnx.LoRAParam),\n)\ncheckpointer = ocp.StandardCheckpointer()\nloaded_params = checkpointer.restore(os.path.join(load_path, \"lora_params\"), target=abs_params)\n\n# Update model with loaded params\nnnx.update(lora_policy, loaded_params)\nprint(\"‚úÖ Trained model loaded!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T18:52:50.616306Z","iopub.execute_input":"2026-01-10T18:52:50.616478Z","iopub.status.idle":"2026-01-10T18:52:52.654054Z","shell.execute_reply.started":"2026-01-10T18:52:50.616463Z","shell.execute_reply":"2026-01-10T18:52:52.652802Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Post-Training Evaluation\n\nEvaluate the fine-tuned model using `evaluate_safe()` which handles variable-length prompts properly.\n\nThis creates a fresh sampler for each question to avoid cache size issues.","metadata":{}},{"cell_type":"code","source":"# EVEN SAFER EVALUATION - handles variable length prompts\nimport os\nos.environ['WANDB_MODE'] = 'disabled'\n\ndef evaluate_safe(dataset, num_samples=20):\n    corr_format = 0\n    corr_answer = 0\n    total = 0\n    \n    for i, item in enumerate(tqdm(dataset[:num_samples])):\n        question = item['prompt']\n        ref_answer = item['answer']\n        prompt = TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=question)\n        \n        # Create fresh sampler for each question to avoid cache issues\n        try:\n            temp_sampler = sampler_lib.Sampler(\n                transformer=lora_policy,\n                tokenizer=tokenizer,\n                cache_config=sampler_lib.CacheConfig(\n                    cache_size=768,\n                    num_layers=model_config.num_layers,\n                    num_kv_heads=model_config.num_kv_heads,\n                    head_dim=model_config.head_dim,\n                ),\n            )\n            \n            out = temp_sampler(\n                input_strings=[prompt],\n                max_generation_steps=200,\n                temperature=0.0,\n            )\n            response = out.text[0] if out.text else \"\"\n            del temp_sampler\n            \n        except Exception as e:\n            print(f\"Skip {i}: {str(e)[:50]}\")\n            continue\n        \n        # Check format and answer\n        has_format = '<reasoning>' in response and '</reasoning>' in response and '<answer>' in response and '</answer>' in response\n        if has_format:\n            corr_format += 1\n        \n        match = re.search(r'<answer>\\s*([A-Da-d])\\s*</answer>', response)\n        if match and match.group(1).upper() == ref_answer.upper():\n            corr_answer += 1\n        \n        total += 1\n        \n        # Show first 3\n        if i < 3:\n            extracted = match.group(1).upper() if match else \"None\"\n            print(f\"\\nQ{i+1}: {question[:60]}...\")\n            print(f\"Expected: {ref_answer} | Got: {extracted} | {'‚úÖ' if match and match.group(1).upper() == ref_answer.upper() else '‚ùå'}\")\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"RESULTS: {total} samples\")\n    print(f\"Format: {100*corr_format/total:.1f}% | Answer: {100*corr_answer/total:.1f}%\")\n    return corr_format, corr_answer, total\n\n# Run it\nprint(\"Evaluating...\")\nevaluate_safe(test_dataset, num_samples=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T18:52:52.654693Z","iopub.execute_input":"2026-01-10T18:52:52.654861Z","iopub.status.idle":"2026-01-10T19:00:57.703163Z","shell.execute_reply.started":"2026-01-10T18:52:52.654847Z","shell.execute_reply":"2026-01-10T19:00:57.702189Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Final Results Summary\n\nSummary of the GRPO training results on ARC science questions.","metadata":{}},{"cell_type":"code","source":"# FINAL SUMMARY FOR SUBMISSION\nprint(\"=\"*60)\nprint(\"ARC SCIENCE QUESTION GRPO TRAINING - FINAL RESULTS\")\nprint(\"=\"*60)\nprint(f\"\"\"\nDataset: ARC (AI2 Reasoning Challenge) - Science Multiple Choice\nModel: Gemma-2 2B IT with LoRA (rank=64)\nTraining: GRPO with custom reward functions\n- Format reward (reasoning + answer tags)\n- Answer correctness reward\n\nRESULTS:\n--------\nFormat Accuracy: 80.0%\nAnswer Accuracy: 55.0%\n\nThe model successfully learned to:\n1. Use <reasoning> tags for step-by-step analysis\n2. Use <answer> tags for final answer\n3. Analyze multiple choice options\n4. Select correct answers at 55% accuracy (vs 25% random baseline)\n\"\"\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T19:00:57.7035Z","iopub.execute_input":"2026-01-10T19:00:57.70369Z","iopub.status.idle":"2026-01-10T19:00:57.70747Z","shell.execute_reply.started":"2026-01-10T19:00:57.703674Z","shell.execute_reply":"2026-01-10T19:00:57.706778Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Showcase Examples\n\nLet's see some example outputs from our trained model to demonstrate the quality of reasoning it has learned.","metadata":{}},{"cell_type":"code","source":"# Generate a few showcase examples\nshowcase_questions = [\n    \"Which is a renewable resource?\\n\\nA: coal\\nB: oil\\nC: sunlight\\nD: natural gas\",\n    \"What is the main function of the heart?\\n\\nA: to digest food\\nB: to pump blood\\nC: to filter air\\nD: to produce hormones\",\n]\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"SAMPLE MODEL OUTPUTS\")\nprint(\"=\"*60)\n\nfor q in showcase_questions:\n    prompt = TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=q)\n    \n    temp_sampler = sampler_lib.Sampler(\n        transformer=lora_policy,\n        tokenizer=tokenizer,\n        cache_config=sampler_lib.CacheConfig(\n            cache_size=768,\n            num_layers=model_config.num_layers,\n            num_kv_heads=model_config.num_kv_heads,\n            head_dim=model_config.head_dim,\n        ),\n    )\n    \n    out = temp_sampler(input_strings=[prompt], max_generation_steps=200, temperature=0.0)\n    \n    print(f\"\\nQuestion: {q}\\n\")\n    print(f\"Model Response:\\n{out.text[0]}\")\n    print(\"-\"*60)\n    del temp_sampler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T19:00:57.707939Z","iopub.execute_input":"2026-01-10T19:00:57.708094Z","iopub.status.idle":"2026-01-10T19:01:45.366482Z","shell.execute_reply.started":"2026-01-10T19:00:57.70808Z","shell.execute_reply":"2026-01-10T19:01:45.365472Z"}},"outputs":[],"execution_count":null}]}